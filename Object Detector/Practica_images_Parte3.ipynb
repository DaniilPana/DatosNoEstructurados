{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Imagen - Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install opencv-python\n",
    "#!pip install keras\n",
    "#!pip install imageAi\n",
    "#!mkdir input models output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Object detector\n",
    "\n",
    "En este caso se ha automatizado el proceso de etiquetado, de imagenes para el _object detector_, el cual consiste en poner un recuadro dentro de la imagen del objeto que se desea catalogar y ponerle una etiqueta-nombre de clase. Puesto que todas las imágenes son del mismo tamaño, se ha podido automatizar el proceso de etiquetado (que al final consiste en tener la información de la etiqueta asignada en un XML). Todo este pre procesado viene explicado a continuación en ingles (he optado por tenerlo en inglés así puedo conservarlo en mi GitHub y tenerlo como muestra de trabajo que he hecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de :https://medium.com/deepquestai/train-object-detection-ai-with-6-lines-of-code-6d087063f6ff\n",
    "\n",
    "Programa para etiquetar: https://medium.com/deepquestai/object-detection-training-preparing-your-custom-dataset-6248679f0d1d\n",
    "\n",
    "\n",
    "\n",
    "## This will be done for train and validation folders\n",
    "- First of all decide what fruits you will use (I use less and less pictures due to processor issues) and the number of pictures to get from the dataset (min 200 per class is recomended)\n",
    "- Then, as images are spread in different directories (for example: Apple Red, Apple Golden) I group them in one sigle directory (Apple) and as inside those directories image filenames are repeated, I rename the files as follows {fruit}{number}\n",
    "- After that, all fruits used are moved into one single directory and format _.jpg_ is added to filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - images/train/images folder (not tagging yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use only 3 fruits due to processor issues: apples, because they are in almost every test picture that contians multiple fruits; grapes because I want to see if the detector will tag every single grape or only one; bananas (same reason as grapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'fruits360/Training/'\n",
    "target_dir = 'images/train/images/'\n",
    "\n",
    "#These are all classes of fruits, without considering their subclass (GOLDEN apple)\n",
    "#classes=([\"Apple\",\"Apricot\",\"Avocado\",\"Banana\",\"Beetroot\",\"Cactus\",\"Cantaloupe\",\"Cauliflower\",\"Cherry\",\"Chestnut\",\"Clementine\",\"Cocos\",\"Corn\",\n",
    "#         \"Cucumber\",\"Eggplant\",\"Fig\",\"Ginger\",\"Granadilla\",\"Grape\",\"Grapefruit\",\"Guava\",\"Hazelnut\",\"Kaki\",\"Kiwi\",\"Kohlrabi\",\"Kumquats\",\"Lemon\",\"Limes\",\n",
    "#         \"Lychee\",\"Mandarine\",\"Mango\",\"Mangostan\",\"Maracuja\",\"Mulberry\",\"Nectarine\",\"Nut\",\"Onion\",\"Orange\",\"Papaya\",\"Passion\",\"Peach\",\"Pear\",\"Pepino\",\n",
    "#         \"Pepper\",\"Physalis\",\"Pineapple\",\"Pitahaya\",\"Plum\",\"Pomelo\",\"Potato\",\"Quince\",\"Raspberry\",\"Rambutan\",\"Redcurrant\",\"Salak\",\"Strawberry\",\"Tamarillo\",\"Tangelo\",\n",
    "#         \"Tomato\",\"Walnut\",\"Watermelon\"]\n",
    "#        )\n",
    "\n",
    "\n",
    "fruits_to_be_used=[\"Apple\",\"Banana\",\"Grape\"]\n",
    "n_samples=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I want 'n_samples', and there are N multiple subclasses of the same fruit, I want to do it as precise as I can and split those 'n_samples' into N\n",
    "\n",
    "Fruits to be used is auto-explicative; n_subsamples is the number of samples that will be taken from each subclass directory (if n_samples = 200 and \n",
    "there are 2 classes of Apples, then n_subsamples = 100); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As I want 'n_samples', and there are N multiple subclasses of the same fruit, I want to do it as precise as I can and split those 'n_samples' into N\n",
    "#Fruits to be used is auto-explicative; n_subsamples is the number of samples that will be taken from each subclass directory (if n_samples = 200 and \n",
    "#there are 2 classes of Apples, then n_subsamples = 100); \n",
    "\n",
    "fruit_info=pd.DataFrame(columns=['fruits_to_be_used','n_subsamples'])\n",
    "fruit_info.fruits_to_be_used=fruits_to_be_used\n",
    "\n",
    "#for 'n_subsample'...\n",
    "#I substract the subclass of the directories (Apple directory will appear in that list as many times as subclasses there are)\n",
    "n_subsamples=[]\n",
    "fruits_subclasses= [x.split(\" \")[0] for x in os.listdir(source_dir)]\n",
    "for i in fruits_to_be_used:\n",
    "    n_subsamples.append(fruits_subclasses.count(i))\n",
    "fruit_info.n_subsamples=n_subsamples\n",
    "\n",
    "fruit_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping different variants of one fruit in one folder and renaming them so I can automate in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping fruits by their main class in separeted directories and adding '.jpg' format\n",
    "\n",
    "#For every listed folder is a class/subclass of fruit that contains images\n",
    "for i in  np.array(os.listdir(source_dir)):\n",
    "    try:\n",
    "        fruit_name=i.split(\" \")[0] #First element of the directory name is always the fruit name (APPLE golden)\n",
    "        if fruit_name in fruits_to_be_used: #If this is the fruit we want...\n",
    "            fruits = np.array(os.listdir(source_dir+i)) #Now we read all the fruits that are inside the directory\n",
    "            n_fruits=np.arange(0,fruits.shape[0]) #To know how many fruits there are inside and also to assign a number to new filename\n",
    "            \n",
    "            os.mkdir(target_dir+fruit_name) #Creating new folder for grouping all fruits of one class (Apple, for example)\n",
    "            #If its OK (not exists already - means that first subclass directory is being read)\n",
    "            \n",
    "            threshhold=n_samples//fruit_info[fruit_info['fruits_to_be_used']==fruit_name].n_subsamples.item()+1 ## Threshold is n_samples/X +1 \n",
    "            for index, value in enumerate(fruits): #Para cada una de las frutas dentro de dicho directorio, la movemos al directorio nuevo\n",
    "                if n_fruits[index]< threshhold:\n",
    "                    shutil.copyfile(source_dir+i+\"/\"+value, target_dir+fruit_name+\"/\"+fruit_name+str(n_fruits[index])+'.jpg') #New name {fruit}{number}\n",
    "    except FileExistsError:\n",
    "        #If folder already exists, means that there are already some fruit images inside therefore the number will not be 0\n",
    "        #Si ya existe la carpeta es que el número que le tengo que asignar a la fruta no empieza en 0\n",
    "        n_fruits_before= np.array(os.listdir(target_dir+fruit_name+\"/\")).shape[0]\n",
    "        n_fruits_new=np.arange(n_fruits_before,n_fruits_before+fruits.shape[0])\n",
    "        threshhold=n_samples//fruit_info[fruit_info['fruits_to_be_used']==fruit_name].n_subsamples.item()+1 ## Threshold is n_samples/X +1 \n",
    "        for index, value in enumerate(fruits): #Para cada una de las frutas dentro de dicho directorio, la movemos al directorio nuevo\n",
    "            if n_fruits[index]< threshhold:\n",
    "                shutil.copyfile(source_dir+i+\"/\"+value, target_dir+fruit_name+\"/\"+fruit_name+str(n_fruits_new[index])+'.jpg') #Le damos un nombre nuevo que es {fruta}{número}\n",
    "    except NotADirectoryError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all fruits have the right name, we just shuffle them all into one folder for training, no longer splitten in folders per fruit type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I'd really like to have fruits in images/train folder and not inside different folders like images/train/Apple\n",
    "source_dir='images/train/images/'\n",
    "target_dir='images/train/images/'\n",
    "for i in  np.array(os.listdir(source_dir)):\n",
    "    try:\n",
    "        fruits = np.array(os.listdir(source_dir+i)) #Read fruits inside directory\n",
    "        for index, value in enumerate(fruits): #Every fruit is moved\n",
    "                shutil.move(source_dir+i+\"/\"+value, target_dir+value) \n",
    "        #After moving all fruits, the old directory is removed -> well, I actually couldnt due to permissions issues and I did not fix it. Do it manually\n",
    "        #os.remove(source_dir+i)\n",
    "    except NotADirectoryError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you want to delete some images randomly\n",
    "\n",
    "#from random import sample \n",
    "#files=np.array(os.listdir('images/train/images'))\n",
    "#num_files=files.shape[0]\n",
    "\n",
    "#randomlist = random.sample(range(1, num_files), num_files//2)\n",
    "#for i in randomlist:\n",
    "#    os.remove('images/train/images/'+files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation - images/validation/images folder\n",
    "Now folder images/validation folder is going to be set up. We will repeat the same process as before and use the same number of validation samples, in terms of imags manipulation. All code will be the same, but image names will include a suffix *_validation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'fruits360/Test/'\n",
    "target_dir = 'images/validation/images/'\n",
    "\n",
    "#These are all classes of fruits, without considering their subclass (GOLDEN apple)\n",
    "#classes=([\"Apple\",\"Apricot\",\"Avocado\",\"Banana\",\"Beetroot\",\"Cactus\",\"Cantaloupe\",\"Cauliflower\",\"Cherry\",\"Chestnut\",\"Clementine\",\"Cocos\",\"Corn\",\n",
    "#         \"Cucumber\",\"Eggplant\",\"Fig\",\"Ginger\",\"Granadilla\",\"Grape\",\"Grapefruit\",\"Guava\",\"Hazelnut\",\"Kaki\",\"Kiwi\",\"Kohlrabi\",\"Kumquats\",\"Lemon\",\"Limes\",\n",
    "#         \"Lychee\",\"Mandarine\",\"Mango\",\"Mangostan\",\"Maracuja\",\"Mulberry\",\"Nectarine\",\"Nut\",\"Onion\",\"Orange\",\"Papaya\",\"Passion\",\"Peach\",\"Pear\",\"Pepino\",\n",
    "#         \"Pepper\",\"Physalis\",\"Pineapple\",\"Pitahaya\",\"Plum\",\"Pomelo\",\"Potato\",\"Quince\",\"Raspberry\",\"Rambutan\",\"Redcurrant\",\"Salak\",\"Strawberry\",\"Tamarillo\",\"Tangelo\",\n",
    "#         \"Tomato\",\"Walnut\",\"Watermelon\"]\n",
    "#        )\n",
    "\n",
    "\n",
    "fruits_to_be_used=[\"Apple\",\"Banana\",\"Grape\"]\n",
    "n_samples=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruits_to_be_used</th>\n",
       "      <th>n_subsamples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Banana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grape</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fruits_to_be_used  n_subsamples\n",
       "0             Apple            13\n",
       "1            Banana             3\n",
       "2             Grape             6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_info=pd.DataFrame(columns=['fruits_to_be_used','n_subsamples'])\n",
    "fruit_info.fruits_to_be_used=fruits_to_be_used\n",
    "\n",
    "#for 'n_subsample'...\n",
    "#I substract the subclass of the directories (Apple directory will appear in that list as many times as subclasses there are)\n",
    "n_subsamples=[]\n",
    "fruits_subclasses= [x.split(\" \")[0] for x in os.listdir(source_dir)]\n",
    "for i in fruits_to_be_used:\n",
    "    n_subsamples.append(fruits_subclasses.count(i))\n",
    "fruit_info.n_subsamples=n_subsamples\n",
    "\n",
    "fruit_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping fruits by their main class in separeted directories and adding '.jpg' format\n",
    "\n",
    "#For every listed folder is a class/subclass of fruit that contains images\n",
    "for i in  np.array(os.listdir(source_dir)):\n",
    "    try:\n",
    "        fruit_name=i.split(\" \")[0] #First element of the directory name is always the fruit name (APPLE golden)\n",
    "        if fruit_name in fruits_to_be_used: #If this is the fruit we want...\n",
    "            fruits = np.array(os.listdir(source_dir+i)) #Now we read all the fruits that are inside the directory\n",
    "            n_fruits=np.arange(0,fruits.shape[0]) #To know how many fruits there are inside and also to assign a number to new filename\n",
    "            \n",
    "            os.mkdir(target_dir+fruit_name) #Creating new folder for grouping all fruits of one class (Apple, for example)\n",
    "            #If its OK (not exists already - means that first subclass directory is being read)\n",
    "            \n",
    "            threshhold=n_samples//fruit_info[fruit_info['fruits_to_be_used']==fruit_name].n_subsamples.item()+1 ## Threshold is n_samples/X +1 \n",
    "            for index, value in enumerate(fruits): #Para cada una de las frutas dentro de dicho directorio, la movemos al directorio nuevo\n",
    "                if n_fruits[index]< threshhold:\n",
    "                    shutil.copyfile(source_dir+i+\"/\"+value, target_dir+fruit_name+\"/\"+fruit_name+'_validation'+str(n_fruits[index])+'.jpg') #New name {fruit}{number}\n",
    "    except FileExistsError:\n",
    "        #If folder already exists, means that there are already some fruit images inside therefore the number will not be 0\n",
    "        #Si ya existe la carpeta es que el número que le tengo que asignar a la fruta no empieza en 0\n",
    "        n_fruits_before= np.array(os.listdir(target_dir+fruit_name+\"/\")).shape[0]\n",
    "        n_fruits_new=np.arange(n_fruits_before,n_fruits_before+fruits.shape[0])\n",
    "        threshhold=n_samples//fruit_info[fruit_info['fruits_to_be_used']==fruit_name].n_subsamples.item()+1 ## Threshold is n_samples/X +1 \n",
    "        for index, value in enumerate(fruits): #Para cada una de las frutas dentro de dicho directorio, la movemos al directorio nuevo\n",
    "            if n_fruits[index]< threshhold:\n",
    "                shutil.copyfile(source_dir+i+\"/\"+value, target_dir+fruit_name+\"/\"+fruit_name+'_validation'+str(n_fruits_new[index])+'.jpg') #Le damos un nombre nuevo que es {fruta}{número}\n",
    "    except NotADirectoryError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I'd really like to have fruits in images/train folder and not inside different folders like images/train/Apple\n",
    "source_dir='images/validation/images/'\n",
    "target_dir='images/validation/images/'\n",
    "for i in  np.array(os.listdir(source_dir)):\n",
    "    try:\n",
    "        fruits = np.array(os.listdir(source_dir+i)) #Read fruits inside directory\n",
    "        for index, value in enumerate(fruits): #Every fruit is moved\n",
    "                shutil.move(source_dir+i+\"/\"+value, target_dir+value) \n",
    "        #After moving all fruits, the old directory is removed -> well, I actually couldnt due to permissions issues and I did not fix it. Do it manually\n",
    "        #os.remove(source_dir+i)\n",
    "    except NotADirectoryError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For automatiting generation of XML from LabelIMG program, that contains tags-labels in each picture + some other info.\n",
    "\n",
    "I CONSIDER that all images, after reviewing some of them, (they do, if Im not wrong, they are all small squared ones). I manually generate one XML and then I manipulate it to generate more copies. As images are small ones, the tag is the whole image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora resulta que quiero automatizar la generación del fichro xml que contiene los datos de la imagen que ha sido taggeada\n",
    "# Tengo que leer el xml y modificar algunos campos. Por suerte el tamaño de la imagen es el mismo en todos los casos, o al menos lo asumo \n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Información de la librería: https://docs.python.org/3/library/xml.etree.elementtree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an Element, root has a tag and a dictionary of attributes and also has child nodes over which we can iterate:\n",
    "#Reading the XML that will be used as template\n",
    "tree = ET.parse('images/train/annotations/Apple0.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "##Python does not show the XML content but it is containted as far as I could see after doing some testing\n",
    "\n",
    "for child in root:\n",
    "    print(child.tag,child.attrib)\n",
    "    \n",
    "# Me interesa tocar el campo 'filename' para adaptarlo a cada imagen, 'path' y 'object -> name', que dentro tiene el tag especificado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Automating XML generation - images/train/annotations folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automate the XML generated by the program that tags the fruits, I manipulate some fields of that XML: filename, path and name (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the current directory to append to it its suffix (the rest of its path)\n",
    "import xml.etree.ElementTree as ET\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML that is going to be our template\n",
    "tree = ET.parse('images/train/annotations/Apple0.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "source_dir=pwd+'images/train/images/'\n",
    "target_dir=pwd+'images/train/annotations/'\n",
    "\n",
    "#I need to know the fruit name because it is going to be the tag-label\n",
    "images = np.array(os.listdir(source_dir))\n",
    "\n",
    "#For every image we will generate a new xml, based on the one we read before because they share the same architecture\n",
    "for filename in images:\n",
    "    fruit_number=filename.split(\".\")[0]\n",
    "    match = re.match(r\"([a-zA-Z]+)\", fruit_number, re.I)\n",
    "    if match:\n",
    "        fruit_name= match.groups()[0]\n",
    "    ## Fields that need to be updated:  'filename', 'path', and 'name' (inside 'object' tag)\n",
    "    #In order to have access to the content of the tag, we use '.text'\n",
    "    root.find('filename').text=filename\n",
    "    root.find('path').text=source_dir+filename\n",
    "    root.find('object').find('name').text=fruit_name\n",
    "    tree.write(target_dir+fruit_number+'.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HIDDEN FILES: If you use MAC, more likely you will have those files messing up in the directory, what will cause you an error while executing the model\n",
    "#Folders are 'images' and dont forget about 'annotations'\n",
    "\n",
    "#!mv images/train/images/.DS_Store .\n",
    "#!rm -rf images/train/images/.ipynb_checkpoints/ \n",
    "\n",
    "#!rm -r images/train/images/.*\n",
    "#!rm -r images/train/annotations/.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation - Automating XML generation - images/validation/annotations folder\n",
    "\n",
    "Idem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the current directory to append to it its suffix (the rest of its path)\n",
    "import xml.etree.ElementTree as ET\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML that is going to be our template\n",
    "tree = ET.parse('images/validation/annotations/Apple_validation0.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "source_dir=pwd+'images/validation/images/'\n",
    "target_dir=pwd+'images/validation/annotations/'\n",
    "\n",
    "#I need to know the fruit name because it is going to be the tag-label\n",
    "images = np.array(os.listdir(source_dir))\n",
    "\n",
    "#For every image we will generate a new xml, based on the one we read before because they share the same architecture\n",
    "for filename in images:\n",
    "    fruit_number=filename.split(\".\")[0]\n",
    "    match = re.match(r\"([a-zA-Z]+)\", fruit_number, re.I)\n",
    "    if match:\n",
    "        fruit_name= match.groups()[0]\n",
    "    ## Fields that need to be updated:  'filename', 'path', and 'name' (inside 'object' tag)\n",
    "    #In order to have access to the content of the tag, we use '.text'\n",
    "    root.find('filename').text=filename\n",
    "    root.find('path').text=source_dir+filename\n",
    "    root.find('object').find('name').text=fruit_name\n",
    "    tree.write(target_dir+fruit_number+'.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf images/validation/images/.*\n",
    "#!rm -rf images/validation/annotations/.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detector\n",
    "\n",
    "Download model from: https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código para revisar que no quedan ficheros ocultos que fastidian el entrenamiento\n",
    "\n",
    "\n",
    "#import cv2\n",
    "#import numpy as np\n",
    "#import os\n",
    "#source1='images/train/annotations/'\n",
    "#source2='images/train/images/'\n",
    "\n",
    "#for i in np.array(os.listdir(source)):\n",
    "#    print(i)\n",
    "#    if i[0]=='.':\n",
    "#        print(i)\n",
    "    #If not sure, execute this two using images as input. 2nd one fails is a non picture is read\n",
    "    #img = cv2.imread(source1+i)\n",
    "    #image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is trained only with 15 epochs and batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 1.00\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  /Users/danipanasik/Desktop/objectDetector/images/json/detection_config.json\n",
      "Evaluating over 613 samples taken from /Users/danipanasik/Desktop/objectDetector/images/validation\n",
      "Training over 613 samples  given at /Users/danipanasik/Desktop/objectDetector/images/train\n",
      "Training on: \t['Apple', 'Banana', 'Grape']\n",
      "Training with Batch Size:  8\n",
      "Number of Training Samples:  613\n",
      "Number of Validation Samples:  613\n",
      "Number of Experiments:  3\n",
      "Training with transfer learning from pretrained Model\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer YoloLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/3\n",
      "616/616 [==============================] - 6767s 11s/step - loss: 76.2967 - yolo_layer_9_loss: 15.7747 - yolo_layer_10_loss: 14.6543 - yolo_layer_11_loss: 34.3309 - val_loss: 20.2930 - val_yolo_layer_9_loss: 9.2838 - val_yolo_layer_10_loss: 0.0306 - val_yolo_layer_11_loss: 0.3579\n",
      "Epoch 2/3\n",
      "616/616 [==============================] - 8004s 13s/step - loss: 15.6186 - yolo_layer_9_loss: 5.6131 - yolo_layer_10_loss: 0.0019 - yolo_layer_11_loss: 0.0844 - val_loss: 12.9907 - val_yolo_layer_9_loss: 4.9053 - val_yolo_layer_10_loss: 2.5803e-05 - val_yolo_layer_11_loss: 7.8606e-04\n",
      "Epoch 3/3\n",
      "616/616 [==============================] - 7299s 12s/step - loss: 11.7220 - yolo_layer_9_loss: 4.0568 - yolo_layer_10_loss: 4.4268e-05 - yolo_layer_11_loss: 2.0306e-04 - val_loss: 10.9003 - val_yolo_layer_9_loss: 4.3229 - val_yolo_layer_10_loss: 3.6111e-06 - val_yolo_layer_11_loss: 5.7546e-05\n"
     ]
    }
   ],
   "source": [
    "#Nos descargamos un modelo preentrenado: https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5\n",
    "\n",
    "#classes=([\"Apple\",\"Apricot\",\"Avocado\",\"Banana\",\"Beetroot\",\"Cactus\",\"Cantaloupe\",\"Cauliflower\",\"Cherry\",\"Chestnut\",\"Clementine\",\"Cocos\",\"Corn\",\n",
    "#         \"Cucumber\",\"Eggplant\",\"Fig\",\"Ginger\",\"Granadilla\",\"Grape\",\"Grapefruit\",\"Guava\",\"Hazelnut\",\"Kaki\",\"Kiwi\",\"Kohlrabi\",\"Kumquats\",\"Lemon\",\"Limes\",\n",
    "#         \"Lychee\",\"Mandarine\",\"Mango\",\"Mangostan\",\"Maracuja\",\"Mulberry\",\"Nectarine\",\"Nut\",\"Onion\",\"Orange\",\"Papaya\",\"Passion\",\"Peach\",\"Pear\",\"Pepino\",\n",
    "#         \"Pepper\",\"Physalis\",\"Pineapple\",\"Pitahaya\",\"Plum\",\"Pomelo\",\"Potato\",\"Quince\",\"Raspberry\",\"Rambutan\",\"Redcurrant\",\"Salak\",\"Strawberry\",\"Tamarillo\",\"Tangelo\",\n",
    "#         \"Tomato\",\"Walnut\",\"Watermelon\"]\n",
    "#        )\n",
    "\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\"\n",
    "fruits_to_be_used=[\"Apple\",\"Banana\",\"Grape\"]\n",
    "\n",
    "\n",
    "#— object_names_array: This is an array of the names of all the objects in your dataset. \n",
    "#— batch_size: This is the batch size for the training. Kindly note that the larger the batch size, the better the detection accuracy of the saved models. \n",
    "#However, due to memory limits on the Nvidia K80 GPU available on Colab, we have to keep this value as 4. The batch size can be values of 8, 16 and so on.\n",
    "#— num_experiments: This is the number of times we want the training code to iterate on our custom dataset.\n",
    "#— train_from_pretrained_model: This is used to leverage transfer learning using the pretrained YOLOv3 model we downloaded earlier.\n",
    "\n",
    "\n",
    "#Once the training starts,\n",
    "#ImageAI will generate detection_config.json file in the hololens/json folder. This JSON file will be used during detection of objects in images and videos\n",
    "#ImageAI will create hololens/models folder which is where all generated models will be saved\n",
    "#You will see at the log like the sample details below.\n",
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=pwd+\"images\")\n",
    "trainer.setTrainConfig(object_names_array=fruits_to_be_used, batch_size=8, num_experiments=15, train_from_pretrained_model=\"pretrained-yolov3.h5\")\n",
    "\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that you pick the best model for your custom detection, ImageAI allows you to evaluate the mAP of all the trained models saved in the images/models folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model evaluation....\n",
      "Evaluating over 613 samples taken from images/validation\n",
      "Training over 613 samples  given at images/train\n",
      "skipping the evaluation of images/models/.ipynb_checkpoints since it's not a .h5 file\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-001--loss-0040.397.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.6683\n",
      "Banana: 0.3933\n",
      "Grape: 0.6278\n",
      "mAP: 0.5632\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-002--loss-0014.567.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.6897\n",
      "Banana: 0.7476\n",
      "Grape: 0.8618\n",
      "mAP: 0.7664\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-003--loss-0011.311.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.4054\n",
      "Banana: 0.8716\n",
      "Grape: 0.7687\n",
      "mAP: 0.6819\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_file': 'images/models/detection_model-ex-001--loss-0040.397.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.6683198114879976,\n",
       "   'Banana': 0.3933307729121957,\n",
       "   'Grape': 0.6278112712750261},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.5631539518917398},\n",
       " {'model_file': 'images/models/detection_model-ex-002--loss-0014.567.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.6896510270232175,\n",
       "   'Banana': 0.7475989740306708,\n",
       "   'Grape': 0.8618257709960145},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.7663585906833009},\n",
       " {'model_file': 'images/models/detection_model-ex-003--loss-0011.311.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.40537118941472233,\n",
       "   'Banana': 0.871642620762183,\n",
       "   'Grape': 0.7687076517919416},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.6819071539896155}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ensure that you pick the best model for your custom detection, ImageAI allows you to evaluate the mAP of all the trained models saved \n",
    "# in the images/models folder.\n",
    "\n",
    "#The higher the mAP, the better the detection accuracy of the model.\n",
    "\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\"\n",
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"images\")\n",
    "trainer.evaluateModel(model_path=\"images/models\", json_path=\"images/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)\n",
    "\n",
    "#In the first 4 lines, we import the same training class, created the class instance, set the detection model type and and set the path to our dataset’s directory.\n",
    "#In the 5th line, we called the .evaluateModel function and specified the parameters below\n",
    "#— model_path: This is the path to the folder containing our models. It can also be the filepath to a specific model.\n",
    "#— json_file: This is the path to the detection_config.json file saved during the training.\n",
    "#— iou_threshold: This is our desired minimum Intersection over Union value for the mAP computation. It can be set to values between 0.0 to 1.0\n",
    "#— object_threshold: This is our desired minimum class score for the mAP computation. It can be set to values between 0.0 to 1.0.\n",
    "#— nms_threshold: This is our desired Non-maximum suppression for the mAP computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best one is the 2nd model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained with 3 epochs, therefore, it is not expected to see a good object detector in terms of _detecting_ in the image the fruit in the right spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to check how the model detects the fruits. Same images are used as input + one that contains grapes and another with bananas. Ouput is in the analog output folder: _PARTE3_output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to detect your images\n",
    "\n",
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"images/models/detection_model-ex-003--loss-0011.311.h5\") \n",
    "detector.setJsonPath(\"images/json/detection_config.json\")\n",
    "detector.loadModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, mix of fruits. Apple should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  80.42610883712769  :  [174, 0, 3135, 2247]\n",
      "Apple  :  89.63481187820435  :  [328, 237, 3047, 2463]\n",
      "Grape  :  77.58365273475647  :  [328, 237, 3047, 2463]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model does not define in the right way the square, therefore we are not 100% sure that the apple detected was a correct detection or not\n",
    "Model also finds grapes, but there are no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  95.37512063980103  :  [306, 117, 2994, 3840]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apples1.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apples1.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model again finds grapes, but there are no. Its probably that only 3 epochs were not enough for the model to distinguish apple's shape and grape's shape. \n",
    "\n",
    "**The fact that all images had same size made the model do not understand that a grape is way smaller than an apple**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apples and peaches. Apples should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  92.95088648796082  :  [72, 45, 3564, 2974]\n",
      "Apple  :  94.86083984375  :  [160, 262, 4202, 2760]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apples_peaches1.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apples_peaches1.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apple and grapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple  :  95.50875425338745  :  [123, 729, 2943, 3823]\n",
      "Grape  :  92.77992844581604  :  [123, 729, 2943, 3823]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apple_grape.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apple_grape.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The fact that apples have a higher % probability than grapes makes me thing that the fact that all images had the same size made the model do not learn grape's size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mangos and bananas. Bananas should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  86.75123453140259  :  [81, 72, 3130, 2416]\n",
      "Apple  :  96.18552327156067  :  [0, 228, 3908, 2219]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/mangos_bananas(lady_finger).jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_mangos_bananas(lady_finger).jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model did not learn anything about bananas**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
