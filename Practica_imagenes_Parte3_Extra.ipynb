{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 epoch training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montamos el object detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "source1='images/train/annotations/'\n",
    "source2='images/train/images/'\n",
    "\n",
    "for i in np.array(os.listdir(source)):\n",
    "#    print(i)\n",
    "    if i[0]=='.':\n",
    "        print(i)\n",
    "    #If not sure, execute this two using images as input. 2nd one fails is a non picture is read\n",
    "    #img = cv2.imread(source1+i)\n",
    "    #image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetTrainConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobject_names_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_experiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_from_pretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "'setTrainConfig()' function allows you to set the properties for the training instances. It accepts the following values:\n",
       "\n",
       "- object_names_array , this is an array of the names of the different objects in your dataset\n",
       "- batch_size (optional),  this is the batch size for the training instance\n",
       "- num_experiments (optional),   also known as epochs, it is the number of times the network will train on all the training dataset\n",
       "- train_from_pretrained_model (optional), this is used to perform transfer learning by specifying the path to a pre-trained YOLOv3 model\n",
       "\n",
       ":param object_names_array:\n",
       ":param batch_size:\n",
       ":param num_experiments:\n",
       ":param train_from_pretrained_model:\n",
       ":return:\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.8/site-packages/imageai/Detection/Custom/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"images\")\n",
    "trainer.setTrainConfig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "[Errno 21] Is a directory: '/Users/danipanasik/Desktop/objectDetector/images/train/annotations/.ipynb_checkpoints'\n",
      "Ignore this bad annotation: /Users/danipanasik/Desktop/objectDetector/images/train/annotations/.ipynb_checkpoints\n",
      "Average IOU for 9 anchors: 1.00\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  /Users/danipanasik/Desktop/objectDetector/images/json/detection_config.json\n",
      "Evaluating over 613 samples taken from /Users/danipanasik/Desktop/objectDetector/images/validation\n",
      "Training over 613 samples  given at /Users/danipanasik/Desktop/objectDetector/images/train\n",
      "Training on: \t['Apple', 'Banana', 'Grape']\n",
      "Training with Batch Size:  8\n",
      "Number of Training Samples:  613\n",
      "Number of Validation Samples:  613\n",
      "Number of Experiments:  15\n",
      "Training with transfer learning from pretrained Model\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer YoloLayer has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danipanasik/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "/Users/danipanasik/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "616/616 [==============================] - 6790s 11s/step - loss: 71.9340 - yolo_layer_loss: 15.3796 - yolo_layer_1_loss: 11.6466 - yolo_layer_2_loss: 33.3755 - val_loss: 21.3287 - val_yolo_layer_loss: 10.5413 - val_yolo_layer_1_loss: 0.0126 - val_yolo_layer_2_loss: 0.2666\n",
      "Epoch 2/15\n",
      "616/616 [==============================] - 6466s 10s/step - loss: 15.2200 - yolo_layer_loss: 5.4474 - yolo_layer_1_loss: 3.5309e-04 - yolo_layer_2_loss: 0.0763 - val_loss: 12.3074 - val_yolo_layer_loss: 4.6709 - val_yolo_layer_1_loss: 6.8511e-05 - val_yolo_layer_2_loss: 8.4872e-04\n",
      "Epoch 3/15\n",
      "616/616 [==============================] - 7183s 12s/step - loss: 11.4777 - yolo_layer_loss: 4.1798 - yolo_layer_1_loss: 2.5382e-05 - yolo_layer_2_loss: 1.9413e-04 - val_loss: 10.0460 - val_yolo_layer_loss: 3.7546 - val_yolo_layer_1_loss: 2.5170e-05 - val_yolo_layer_2_loss: 4.7535e-05\n",
      "Epoch 4/15\n",
      "616/616 [==============================] - 6491s 11s/step - loss: 9.1483 - yolo_layer_loss: 3.1655 - yolo_layer_1_loss: 8.7955e-06 - yolo_layer_2_loss: 3.7743e-05 - val_loss: 8.2663 - val_yolo_layer_loss: 3.0663 - val_yolo_layer_1_loss: 6.7551e-05 - val_yolo_layer_2_loss: 0.0030\n",
      "Epoch 5/15\n",
      "616/616 [==============================] - 6452s 10s/step - loss: 7.6710 - yolo_layer_loss: 2.6357 - yolo_layer_1_loss: 5.4265e-06 - yolo_layer_2_loss: 1.4682e-05 - val_loss: 7.8223 - val_yolo_layer_loss: 3.2584 - val_yolo_layer_1_loss: 3.9352e-06 - val_yolo_layer_2_loss: 1.1495e-05\n",
      "Epoch 6/15\n",
      "616/616 [==============================] - 6756s 11s/step - loss: 6.6755 - yolo_layer_loss: 2.2563 - yolo_layer_1_loss: 5.8880e-06 - yolo_layer_2_loss: 9.9609e-06 - val_loss: 7.2630 - val_yolo_layer_loss: 3.2415 - val_yolo_layer_1_loss: 3.2181e-07 - val_yolo_layer_2_loss: 1.3168e-04\n",
      "Epoch 7/15\n",
      "616/616 [==============================] - 6113s 10s/step - loss: 5.9247 - yolo_layer_loss: 2.0247 - yolo_layer_1_loss: 3.1788e-06 - yolo_layer_2_loss: 7.7150e-06 - val_loss: 7.7956 - val_yolo_layer_loss: 4.2386 - val_yolo_layer_1_loss: 5.6039e-07 - val_yolo_layer_2_loss: 2.6318e-05\n",
      "Epoch 8/15\n",
      "616/616 [==============================] - 6512s 11s/step - loss: 5.3837 - yolo_layer_loss: 1.9253 - yolo_layer_1_loss: 1.5712e-05 - yolo_layer_2_loss: 5.6132e-06 - val_loss: 5.8174 - val_yolo_layer_loss: 2.6218 - val_yolo_layer_1_loss: 8.2798e-07 - val_yolo_layer_2_loss: 2.0277e-06\n",
      "Epoch 9/15\n",
      "616/616 [==============================] - 6495s 11s/step - loss: 4.9355 - yolo_layer_loss: 1.8251 - yolo_layer_1_loss: 1.5391e-05 - yolo_layer_2_loss: 4.0858e-06 - val_loss: 5.3365 - val_yolo_layer_loss: 2.4551 - val_yolo_layer_1_loss: 1.3300e-08 - val_yolo_layer_2_loss: 0.0073\n",
      "Epoch 10/15\n",
      "616/616 [==============================] - 6272s 10s/step - loss: 4.4499 - yolo_layer_loss: 1.6524 - yolo_layer_1_loss: 2.0992e-05 - yolo_layer_2_loss: 8.5101e-06 - val_loss: 6.0370 - val_yolo_layer_loss: 2.5366 - val_yolo_layer_1_loss: 0.9204 - val_yolo_layer_2_loss: 1.7252e-06\n",
      "Epoch 11/15\n",
      "616/616 [==============================] - 6257s 10s/step - loss: 4.0526 - yolo_layer_loss: 1.5389 - yolo_layer_1_loss: 1.1740e-05 - yolo_layer_2_loss: 2.9301e-06 - val_loss: 5.3451 - val_yolo_layer_loss: 3.0141 - val_yolo_layer_1_loss: 8.1997e-08 - val_yolo_layer_2_loss: 2.5288e-05\n",
      "Epoch 12/15\n",
      "616/616 [==============================] - 6538s 11s/step - loss: 3.7456 - yolo_layer_loss: 1.4731 - yolo_layer_1_loss: 7.6298e-06 - yolo_layer_2_loss: 4.0459e-05 - val_loss: 6.7921 - val_yolo_layer_loss: 4.6685 - val_yolo_layer_1_loss: 2.1047e-06 - val_yolo_layer_2_loss: 4.1824e-06\n",
      "Epoch 13/15\n",
      "616/616 [==============================] - 7160s 12s/step - loss: 3.5417 - yolo_layer_loss: 1.4647 - yolo_layer_1_loss: 7.2114e-05 - yolo_layer_2_loss: 2.2392e-06 - val_loss: 3.9633 - val_yolo_layer_loss: 2.0247 - val_yolo_layer_1_loss: 5.3032e-07 - val_yolo_layer_2_loss: 1.9207e-06\n",
      "Epoch 14/15\n",
      "616/616 [==============================] - 6762s 11s/step - loss: 3.2910 - yolo_layer_loss: 1.3946 - yolo_layer_1_loss: 5.0487e-05 - yolo_layer_2_loss: 8.9802e-04 - val_loss: 5.1347 - val_yolo_layer_loss: 3.3609 - val_yolo_layer_1_loss: 7.7744e-07 - val_yolo_layer_2_loss: 2.7616e-05\n",
      "Epoch 15/15\n",
      "616/616 [==============================] - 7308s 12s/step - loss: 3.2095 - yolo_layer_loss: 1.4663 - yolo_layer_1_loss: 1.3245e-05 - yolo_layer_2_loss: 1.3412e-06 - val_loss: 3.3919 - val_yolo_layer_loss: 1.7519 - val_yolo_layer_1_loss: 1.3945e-08 - val_yolo_layer_2_loss: 2.8764e-05\n"
     ]
    }
   ],
   "source": [
    "#Nos descargamos un modelo preentrenado: https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5\n",
    "\n",
    "#classes=([\"Apple\",\"Apricot\",\"Avocado\",\"Banana\",\"Beetroot\",\"Cactus\",\"Cantaloupe\",\"Cauliflower\",\"Cherry\",\"Chestnut\",\"Clementine\",\"Cocos\",\"Corn\",\n",
    "#         \"Cucumber\",\"Eggplant\",\"Fig\",\"Ginger\",\"Granadilla\",\"Grape\",\"Grapefruit\",\"Guava\",\"Hazelnut\",\"Kaki\",\"Kiwi\",\"Kohlrabi\",\"Kumquats\",\"Lemon\",\"Limes\",\n",
    "#         \"Lychee\",\"Mandarine\",\"Mango\",\"Mangostan\",\"Maracuja\",\"Mulberry\",\"Nectarine\",\"Nut\",\"Onion\",\"Orange\",\"Papaya\",\"Passion\",\"Peach\",\"Pear\",\"Pepino\",\n",
    "#         \"Pepper\",\"Physalis\",\"Pineapple\",\"Pitahaya\",\"Plum\",\"Pomelo\",\"Potato\",\"Quince\",\"Raspberry\",\"Rambutan\",\"Redcurrant\",\"Salak\",\"Strawberry\",\"Tamarillo\",\"Tangelo\",\n",
    "#         \"Tomato\",\"Walnut\",\"Watermelon\"]\n",
    "#        )\n",
    "import os\n",
    "\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\"\n",
    "fruits_to_be_used=[\"Apple\",\"Banana\",\"Grape\"]\n",
    "\n",
    "\n",
    "#— object_names_array: This is an array of the names of all the objects in your dataset. \n",
    "#— batch_size: This is the batch size for the training. Kindly note that the larger the batch size, the better the detection accuracy of the saved models. \n",
    "#However, due to memory limits on the Nvidia K80 GPU available on Colab, we have to keep this value as 4. The batch size can be values of 8, 16 and so on.\n",
    "#— num_experiments = epochs This is the number of times we want the training code to iterate on our custom dataset.\n",
    "#— train_from_pretrained_model: This is used to leverage transfer learning using the pretrained YOLOv3 model we downloaded earlier.\n",
    "\n",
    "\n",
    "#Once the training starts,\n",
    "#ImageAI will generate detection_config.json file in the hololens/json folder. This JSON file will be used during detection of objects in images and videos\n",
    "#ImageAI will create hololens/models folder which is where all generated models will be saved\n",
    "#You will see at the log like the sample details below.\n",
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=pwd+\"images\")\n",
    "trainer.setTrainConfig(object_names_array=fruits_to_be_used, batch_size=8, num_experiments=15, train_from_pretrained_model=\"pretrained-yolov3.h5\")\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model evaluation....\n",
      "Evaluating over 613 samples taken from images/validation\n",
      "Training over 613 samples  given at images/train\n",
      "skipping the evaluation of images/models/.ipynb_checkpoints since it's not a .h5 file\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-001--loss-0038.016.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.4805\n",
      "Banana: 0.0000\n",
      "Grape: 0.2114\n",
      "mAP: 0.2306\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-001--loss-0040.397.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.6688\n",
      "Banana: 0.3933\n",
      "Grape: 0.6278\n",
      "mAP: 0.5633\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-002--loss-0013.874.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.6709\n",
      "Banana: 0.9871\n",
      "Grape: 0.7522\n",
      "mAP: 0.8034\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-002--loss-0014.567.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.6897\n",
      "Banana: 0.7476\n",
      "Grape: 0.8618\n",
      "mAP: 0.7664\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-003--loss-0010.816.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.3875\n",
      "Banana: 0.0100\n",
      "Grape: 0.7228\n",
      "mAP: 0.3734\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-003--loss-0011.311.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.4085\n",
      "Banana: 0.8716\n",
      "Grape: 0.7687\n",
      "mAP: 0.6829\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-004--loss-0008.799.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.8621\n",
      "Banana: 0.9970\n",
      "Grape: 0.8919\n",
      "mAP: 0.9170\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-005--loss-0007.458.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9876\n",
      "Banana: 0.9882\n",
      "Grape: 0.9928\n",
      "mAP: 0.9895\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-006--loss-0006.525.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 1.0000\n",
      "Banana: 0.9835\n",
      "Grape: 0.9981\n",
      "mAP: 0.9939\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-007--loss-0005.773.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9984\n",
      "Banana: 0.9803\n",
      "Grape: 1.0000\n",
      "mAP: 0.9929\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-008--loss-0005.373.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9902\n",
      "Banana: 0.9982\n",
      "Grape: 0.9491\n",
      "mAP: 0.9792\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-009--loss-0004.835.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.8554\n",
      "Banana: 0.9053\n",
      "Grape: 0.9819\n",
      "mAP: 0.9142\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-010--loss-0004.386.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9194\n",
      "Banana: 0.9801\n",
      "Grape: 1.0000\n",
      "mAP: 0.9665\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-011--loss-0004.037.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9209\n",
      "Banana: 0.9941\n",
      "Grape: 0.9921\n",
      "mAP: 0.9691\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-012--loss-0003.754.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9890\n",
      "Banana: 0.9089\n",
      "Grape: 0.9143\n",
      "mAP: 0.9374\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-013--loss-0003.459.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9931\n",
      "Banana: 0.9991\n",
      "Grape: 0.9818\n",
      "mAP: 0.9913\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-014--loss-0003.224.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9670\n",
      "Banana: 0.9698\n",
      "Grape: 0.9989\n",
      "mAP: 0.9786\n",
      "===============================\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model File:  images/models/detection_model-ex-015--loss-0003.127.h5 \n",
      "\n",
      "Evaluation samples:  613\n",
      "Using IoU:  0.5\n",
      "Using Object Threshold:  0.3\n",
      "Using Non-Maximum Suppression:  0.5\n",
      "Apple: 0.9831\n",
      "Banana: 0.9998\n",
      "Grape: 0.8388\n",
      "mAP: 0.9406\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_file': 'images/models/detection_model-ex-001--loss-0038.016.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.48047830996762747,\n",
       "   'Banana': 0.0,\n",
       "   'Grape': 0.21141556860486976},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.2306312928574991},\n",
       " {'model_file': 'images/models/detection_model-ex-001--loss-0040.397.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.6687527687170713,\n",
       "   'Banana': 0.3933307729121957,\n",
       "   'Grape': 0.6278112712750261},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.5632982709680977},\n",
       " {'model_file': 'images/models/detection_model-ex-002--loss-0013.874.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.6708745318689142,\n",
       "   'Banana': 0.9870742655716194,\n",
       "   'Grape': 0.7522158599156983},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.8033882191187439},\n",
       " {'model_file': 'images/models/detection_model-ex-002--loss-0014.567.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.6896510270232175,\n",
       "   'Banana': 0.7475989740306708,\n",
       "   'Grape': 0.8618257709960145},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.7663585906833009},\n",
       " {'model_file': 'images/models/detection_model-ex-003--loss-0010.816.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.38748648823119397,\n",
       "   'Banana': 0.009950248756218905,\n",
       "   'Grape': 0.7227724269031193},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.3734030546301774},\n",
       " {'model_file': 'images/models/detection_model-ex-003--loss-0011.311.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.4084620187464251,\n",
       "   'Banana': 0.871642620762183,\n",
       "   'Grape': 0.7687076517919416},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.6829374304335166},\n",
       " {'model_file': 'images/models/detection_model-ex-004--loss-0008.799.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.8620928278434026,\n",
       "   'Banana': 0.9969766177473334,\n",
       "   'Grape': 0.8918553285433599},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9169749247113653},\n",
       " {'model_file': 'images/models/detection_model-ex-005--loss-0007.458.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9875825244588022,\n",
       "   'Banana': 0.9881793693556584,\n",
       "   'Grape': 0.9928433278769482},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9895350738971361},\n",
       " {'model_file': 'images/models/detection_model-ex-006--loss-0006.525.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9999542124542125,\n",
       "   'Banana': 0.98350561602468,\n",
       "   'Grape': 0.9981309035524942},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9938635773437956},\n",
       " {'model_file': 'images/models/detection_model-ex-007--loss-0005.773.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9984126073773009,\n",
       "   'Banana': 0.9803395283911525,\n",
       "   'Grape': 1.0},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9929173785894845},\n",
       " {'model_file': 'images/models/detection_model-ex-008--loss-0005.373.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9901983844579818,\n",
       "   'Banana': 0.9982352788481297,\n",
       "   'Grape': 0.9490854325358037},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9791730319473051},\n",
       " {'model_file': 'images/models/detection_model-ex-009--loss-0004.835.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.8553751991728356,\n",
       "   'Banana': 0.9052823315118396,\n",
       "   'Grape': 0.9818563240981062},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9141712849275937},\n",
       " {'model_file': 'images/models/detection_model-ex-010--loss-0004.386.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9193806194848663,\n",
       "   'Banana': 0.9800995024875623,\n",
       "   'Grape': 1.0},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9664933739908096},\n",
       " {'model_file': 'images/models/detection_model-ex-011--loss-0004.037.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9209375815364496,\n",
       "   'Banana': 0.9941077341770903,\n",
       "   'Grape': 0.9921136497526205},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9690529884887201},\n",
       " {'model_file': 'images/models/detection_model-ex-012--loss-0003.754.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9889765970116814,\n",
       "   'Banana': 0.9089210309379259,\n",
       "   'Grape': 0.9143104588728579},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9374026956074885},\n",
       " {'model_file': 'images/models/detection_model-ex-013--loss-0003.459.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9930558782685992,\n",
       "   'Banana': 0.9991129428082595,\n",
       "   'Grape': 0.9817535445159011},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9913074551975866},\n",
       " {'model_file': 'images/models/detection_model-ex-014--loss-0003.224.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9670184910110833,\n",
       "   'Banana': 0.9698446138034413,\n",
       "   'Grape': 0.998886345736318},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9785831501836141},\n",
       " {'model_file': 'images/models/detection_model-ex-015--loss-0003.127.h5',\n",
       "  'using_iou': 0.5,\n",
       "  'using_object_threshold': 0.3,\n",
       "  'using_non_maximum_suppression': 0.5,\n",
       "  'average_precision': {'Apple': 0.9831133404583541,\n",
       "   'Banana': 0.9998279513638086,\n",
       "   'Grape': 0.8387673412508074},\n",
       "  'evaluation_samples': 613,\n",
       "  'map': 0.9405695443576567}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ensure that you pick the best model for your custom detection, ImageAI allows you to evaluate the mAP of all the trained models saved \n",
    "# in the images/models folder.\n",
    "\n",
    "#The higher the mAP, the better the detection accuracy of the model.\n",
    "\n",
    "pwd=os.path.abspath(os.getcwd())+\"/\"\n",
    "\n",
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"images\")\n",
    "trainer.evaluateModel(model_path=\"images/models\", json_path=\"images/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)\n",
    "\n",
    "#In the first 4 lines, we import the same training class, created the class instance, set the detection model type and and set the path to our dataset’s directory.\n",
    "#In the 5th line, we called the .evaluateModel function and specified the parameters below\n",
    "#— model_path: This is the path to the folder containing our models. It can also be the filepath to a specific model.\n",
    "#— json_file: This is the path to the detection_config.json file saved during the training.\n",
    "#— iou_threshold: This is our desired minimum Intersection over Union value for the mAP computation. It can be set to values between 0.0 to 1.0\n",
    "#— object_threshold: This is our desired minimum class score for the mAP computation. It can be set to values between 0.0 to 1.0.\n",
    "#— nms_threshold: This is our desired Non-maximum suppression for the mAP computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best one is e images/models/detection_model-ex-006--loss-0006.525.h5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to detect your images\n",
    "\n",
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"images/models/detection_model-ex-006--loss-0006.525.h5\") \n",
    "detector.setJsonPath(\"images/json/detection_config.json\")\n",
    "detector.loadModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, mix of fruits. Apple should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  80.42610883712769  :  [174, 0, 3135, 2247]\n",
      "Apple  :  89.63481187820435  :  [328, 237, 3047, 2463]\n",
      "Grape  :  77.58365273475647  :  [328, 237, 3047, 2463]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apple_apricot_nectarine_peach_peach(flat)_pomegranate_pear_plum.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with 15 epochs is way better than the one with 3 epochs. The boxes generated start to converge and we see that the model has learnt about bananas (they did not appear in any detection before), but with a lot of error because there are no bananas in this image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  95.37512063980103  :  [306, 117, 2994, 3840]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apples1.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apples1.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has started to learn about bananas and looks like 15 epochs is an iteration where model compensates the weights of nodes in a such way where apples have less importance because in an image where there are only apples, bananas are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apples and peaches. Apples should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  92.95088648796082  :  [72, 45, 3564, 2974]\n",
      "Apple  :  94.86083984375  :  [160, 262, 4202, 2760]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apples_peaches1.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apples_peaches1.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apple and grapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple  :  95.50875425338745  :  [123, 729, 2943, 3823]\n",
      "Grape  :  92.77992844581604  :  [123, 729, 2943, 3823]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/apple_grape.jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_apple_grape.jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mangos and bananas. Bananas should be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana  :  86.75123453140259  :  [81, 72, 3130, 2416]\n",
      "Apple  :  96.18552327156067  :  [0, 228, 3908, 2219]\n"
     ]
    }
   ],
   "source": [
    "input_path=\"input/mangos_bananas(lady_finger).jpg\"\n",
    "output_path = \"PARTE3_output/DETECTED_mangos_bananas(lady_finger).jpg\"\n",
    "detections = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bananas detected and mangos are considered as apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
